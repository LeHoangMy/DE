import asyncio
import aiohttp
import random
import json
import time
import csv
from pathlib import Path
from typing import Dict, Any, List, Tuple, Set

API_URL = "https://api.tiki.vn/product-detail/api/v1/products/{}"

# ========== CONFIG CRAWLING ==========
INPUT_CSV = "../Data/products_200k_tiki.csv"
LIMIT_IDS = 200000 # S·ªë l∆∞·ª£ng ID t·ªëi ƒëa mu·ªën t·∫£i trong m·ªói l·∫ßn ch·∫°y
CONCURRENCY = 50  
MAX_RETRY = 7       
TIMEOUT = 10
BASE_BACKOFF = 2.0 
SAVE_FOLDER = "output_2k"
BATCH_SIZE = 1000   # K√≠ch th∆∞·ªõc Batch (1000 ID OK m·ªói file JSON)
FAIL_ID_FILE = f"{SAVE_FOLDER}/fail_ids.csv"
STATS_RESULT_FILE = f"{SAVE_FOLDER}/stats_result.txt"
# ======================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
}

PRODUCT_QUEUE = asyncio.Queue() 

# --- H√ÄM T·∫¢I V√Ä L·ªåC ID ---

def load_ids(offset_count: int) -> List[int]:
    """T·∫£i LIMIT_IDS t·ª´ file CSV, b·∫Øt ƒë·∫ßu sau offset_count d√≤ng."""
    ids = []
    skipped_count = 0
    
    try:
        with open(INPUT_CSV, "r", encoding="utf-8") as f:
            next(f) # B·ªè qua h√†ng header
            for line in f:
                if skipped_count < offset_count:
                    # B·ªè qua c√°c ID ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (l√†m offset)
                    skipped_count += 1
                    continue
                
                # B·∫Øt ƒë·∫ßu thu th·∫≠p ID
                pid_str = line.strip().split(",")[0]
                if pid_str.isdigit():
                    ids.append(int(pid_str))
                    if len(ids) >= LIMIT_IDS:
                        break
                        
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {INPUT_CSV}. D·ª´ng ch∆∞∆°ng tr√¨nh.")
    return ids


def load_last_partial_batch() -> Tuple[int, List[Dict[str, Any]]]:
    """
    T√¨m file JSON c√≥ index l·ªõn nh·∫•t. N·∫øu n√≥ ch∆∞a ƒë·∫ßy BATCH_SIZE, 
    tr·∫£ v·ªÅ index v√† n·ªôi dung c·ªßa n√≥. Ng∆∞·ª£c l·∫°i, tr·∫£ v·ªÅ index m·ªõi.
    """
    output_path = Path(SAVE_FOLDER)
    max_index = 0
    last_batch_data = []
    last_batch_path = None
    
    for file_path in output_path.glob('products_*.json'):
        try:
            index_str = file_path.stem.split('_')[-1]
            index = int(index_str)
            if index > max_index:
                max_index = index
                last_batch_path = file_path
        except ValueError:
            continue
            
    if last_batch_path and last_batch_path.exists():
        try:
            with open(last_batch_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # N·∫øu file cu·ªëi c√πng ch∆∞a ƒë·∫°t BATCH_SIZE (ch∆∞a ƒë·∫ßy), ta ƒëi·ªÅn v√†o n√≥
            if len(data) < BATCH_SIZE:
                last_batch_data = data
                return max_index, last_batch_data
                
            # N·∫øu file ƒë√£ ƒë·∫ßy, ta b·∫Øt ƒë·∫ßu file m·ªõi (index + 1)
            return max_index + 1, []
            
        except Exception as e:
            print(f"C·∫£nh b√°o: Kh√¥ng th·ªÉ ƒë·ªçc file JSON {last_batch_path}. B·∫Øt ƒë·∫ßu t·ª´ file m·ªõi. {e}")
            
    # N·∫øu kh√¥ng t√¨m th·∫•y file n√†o, b·∫Øt ƒë·∫ßu t·ª´ index 1
    return 1, []


def load_completed_ids(partial_batch_data: List[Dict[str, Any]]) -> Set[int]:
    """T·∫£i t·∫•t c·∫£ ID ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (OK/404/FAIL) t·ª´ c√°c file ƒë·∫ßu ra."""
    completed_ids = set()
    output_path = Path(SAVE_FOLDER)
    
    # 1. T·∫£i ID t·ª´ c√°c file JSON ƒë√£ l∆∞u (ID OK)
    for json_file in output_path.glob('products_*.json'):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    if item and item.get('id') is not None:
                        completed_ids.add(int(item['id']))
        except Exception as e:
            continue
            
    # 2. Lo·∫°i b·ªè ID trong batch cu·ªëi c√πng (v√¨ ta s·∫Ω ghi ƒë√® l·∫°i n√≥)
    for item in partial_batch_data:
        if item and item.get('id') is not None:
            completed_ids.remove(int(item['id']))
            
    # 3. T·∫£i ID t·ª´ file th·∫•t b·∫°i (ID FAIL/404)
    fail_file = Path(FAIL_ID_FILE)
    if fail_file.exists():
        try:
            with open(fail_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                next(reader) 
                for row in reader:
                    if row and row[0].isdigit():
                        completed_ids.add(int(row[0]))
        except Exception as e:
            print(f"C·∫£nh b√°o: Kh√¥ng th·ªÉ ƒë·ªçc file FAIL ID {FAIL_ID_FILE}. {e}")
            
    return completed_ids

# --- H√ÄM TR√çCH XU·∫§T V√Ä FETCH ---

def extract_product_info(data: Dict[str, Any]) -> Dict[str, Any]:
    """Tr√≠ch xu·∫•t c√°c th√¥ng tin c·∫ßn thi·∫øt t·ª´ d·ªØ li·ªáu JSON Tiki."""
    image_urls = []
    if data.get('images'):
        for img in data['images']:
            if img.get('base_url'):
                image_urls.append(img['base_url'])

    return {
        'id': data.get('id'),
        'name': data.get('name'),
        'url_key': data.get('url_key'),
        'price': data.get('price'),
        'description': data.get('description', ''),
        'images_url': image_urls
    }


async def fetch_product(session: aiohttp.ClientSession, pid: int) -> Tuple[Any, str]:
    """Fetch s·∫£n ph·∫©m v√† x·ª≠ l√Ω retry."""
    url = API_URL.format(pid)
    attempt = 0
    error_type = "Unknown"

    while attempt < MAX_RETRY:
        attempt += 1

        try:
            async with session.get(url, timeout=TIMEOUT) as resp:
                status = resp.status

                if status == 200:
                    data = await resp.json()
                    extracted_data = extract_product_info(data)
                    return extracted_data, "OK"

                if status == 404:
                    return None, "404"

                if status == 429:
                    retry_after = resp.headers.get("Retry-After")
                    if retry_after:
                        wait_time = int(float(retry_after))
                    else:
                        wait_time = BASE_BACKOFF ** attempt * random.uniform(1.0, 2.0)

                    await asyncio.sleep(wait_time)
                    continue

                await asyncio.sleep(BASE_BACKOFF ** attempt)
                continue

        except Exception as e:
            error_type = type(e).__name__
            await asyncio.sleep(BASE_BACKOFF ** attempt * random.uniform(1.0, 2.0))

    return None, f"FAIL ({error_type})"


async def save_fail_ids(fail_ids: List[Tuple[int, str]]):
    """L∆∞u danh s√°ch ID th·∫•t b·∫°i v√†o file CSV."""
    if not fail_ids:
        return
        
    mode = 'a'
    with open(FAIL_ID_FILE, mode, newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        if f.tell() == 0:
             writer.writerow(['pid', 'status/error'])
             
        for pid, error in fail_ids:
            writer.writerow([pid, error])


async def worker(name, session, queue, stats: Dict[str, Any]) -> Dict[str, Any]:
    """Worker chuy√™n fetch d·ªØ li·ªáu v√† ƒë·∫©y OK v√†o PRODUCT_QUEUE. 
    L∆∞u 404 v√† FAIL v√†o local_stats['fail_ids']."""
    
    # local_stats s·∫Ω ƒë∆∞·ª£c merge v√†o stats cu·ªëi c√πng
    local_stats = {'ok': 0, '404': 0, 'fail_ids': []}
    
    while True:
        pid = await queue.get()
        if pid is None:
            queue.task_done()
            break

        await asyncio.sleep(random.uniform(0.01, 0.05))

        # G·ªçi h√†m fetch_product ƒë·ªÉ l·∫•y d·ªØ li·ªáu
        data, status = await fetch_product(session, pid)

        if status == "OK":
            local_stats['ok'] += 1
            await PRODUCT_QUEUE.put(data) 
            
        elif status == "404":
            local_stats['404'] += 1
            # üí° FIX: Th√™m ID 404 v√†o danh s√°ch fail_ids ƒë·ªÉ ƒë∆∞·ª£c l∆∞u v√†o CSV
            # L∆∞u v·ªõi tr·∫°ng th√°i "404" ƒë·ªÉ ph√¢n bi·ªát r√µ r√†ng
            local_stats['fail_ids'].append((pid, "404")) 
            
        else: # FAIL (Bao g·ªìm c·∫£ l·ªói 429 li√™n t·ª•c, Timeout, Connection Errors)
            # Tr·∫°ng th√°i status l√∫c n√†y l√† chu·ªói 'FAIL (ErrorType)'
            local_stats['fail_ids'].append((pid, status))
            
            # Ch·ªâ tƒÉng t·ªïng s·ªë l·ªói kh√¥ng ph·ª•c h·ªìi (FAIL) tr√™n to√†n c·ª•c
            async with stats['lock']:
                stats['total_fail'] += 1 

        queue.task_done()
        
    return local_stats


async def batch_saver(stats: Dict[str, Any], initial_index: int, initial_buffer: List[Dict[str, Any]]):
    """
    Worker chuy√™n l∆∞u batch. ƒê·∫£m b·∫£o m·ªói file JSON ƒë·ªß BATCH_SIZE.
    """
    
    batch_buffer = initial_buffer
    batch_index = initial_index 
    batch_time_start = time.time()
    
    if batch_buffer:
        print(f"\n[RESUME] Ti·∫øp t·ª•c ƒëi·ªÅn v√†o Batch {batch_index:03d}. Hi·ªán c√≥ {len(batch_buffer)}/{BATCH_SIZE} ID.")
    
    while True:
        try:
            # D√πng timeout ƒë·ªÉ ki·ªÉm tra xem workers ƒë√£ xong ch∆∞a
            product_data = await asyncio.wait_for(PRODUCT_QUEUE.get(), timeout=1) 
        except asyncio.TimeoutError:
            if stats['workers_done']:
                break
            continue

        batch_buffer.append(product_data)

        # L∆∞u file ch·ªâ khi buffer ƒë·∫°t BATCH_SIZE (1000 ID)
        if len(batch_buffer) == BATCH_SIZE:
            
            file_index = batch_index
            out_file = f"{SAVE_FOLDER}/products_{file_index:03d}.json"

            end_time = time.time()
            elapsed = end_time - batch_time_start
            
            ok_count = len(batch_buffer) 
            
            async with stats['lock']:
                stats['total_ok'] += ok_count
                
                batch_info = {
                    'batch': file_index, 
                    'time': f"{elapsed:.2f}s", 
                    'ok': ok_count,
                    '404': 0, 
                    'fail': 0 
                }
                stats['batches'].append(batch_info)

            with open(out_file, "w", encoding="utf-8") as f:
                json.dump(batch_buffer, f, ensure_ascii=False, indent=2)

            print(f"\n[SAVED - FULL] Batch {file_index} ƒê√É ƒê·∫¶Y ({len(batch_buffer)} IDs) ‚Üí {out_file}")

            # Reset ƒë·ªÉ chu·∫©n b·ªã cho batch ti·∫øp theo
            batch_buffer.clear()
            batch_index += 1
            batch_time_start = time.time()
            
            PRODUCT_QUEUE.task_done()
        else:
            PRODUCT_QUEUE.task_done()

    # X·ª≠ l√Ω batch cu·ªëi c√πng (ch∆∞a ƒë·∫ßy 1000)
    if batch_buffer:
        file_index = batch_index
        out_file = f"{SAVE_FOLDER}/products_{file_index:03d}.json"
        elapsed = time.time() - batch_time_start
        ok_count = len(batch_buffer) 

        async with stats['lock']:
            # T√≠nh s·ªë ID m·ªõi ƒë∆∞·ª£c th√™m v√†o trong l·∫ßn ch·∫°y n√†y
            ok_newly_added = ok_count - len(initial_buffer) if file_index == initial_index else ok_count
            stats['total_ok'] += ok_newly_added

            batch_info = {
                'batch': file_index, 
                'time': f"{elapsed:.2f}s", 
                'ok': ok_count, 
                'ok_new': ok_newly_added, 
                '404': 0, 
                'fail': 0
            }
            stats['batches'].append(batch_info)
        
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(batch_buffer, f, ensure_ascii=False, indent=2)

        print(
            f"\n[SAVED - PARTIAL] Batch {file_index} ({len(batch_buffer)} ID) ‚Üí {out_file}\n"
            f"  ƒê√£ th√™m {ok_newly_added} ID m·ªõi. L∆∞u file d·ªü dang ƒë·ªÉ ch·∫°y ti·∫øp."
        )
    
    return


def format_stats_report(stats: Dict[str, Any], run_mode: str) -> str:
    report = []
    
    if run_mode == "APPEND":
        report.append("\n\n" + "="*50)
        report.append("  B√ÅO C√ÅO CH·∫†Y TI·∫æP T·ª§C (APPEND MODE)")
        report.append("="*50)
    else:
        report.append("="*50)
        report.append("       B√ÅO C√ÅO CRAWL CU·ªêI C√ôNG (NEW RUN)")
        report.append("="*50)

    report.append(f"T·ªïng s·ªë ID ƒë√£ ho√†n th√†nh (Offset): {stats['completed_ids_on_start']} ID")
    report.append(f"S·ªë ID c·∫ßn x·ª≠ l√Ω trong l·∫ßn ch·∫°y n√†y: {stats['total_ids']}")
    report.append(f"T·ªïng th·ªùi gian ch·∫°y: {stats['total_time']:.2f} gi√¢y")
    report.append("-" * 50)
    report.append(f"‚úÖ ID OK (L∆∞u Data): {stats['total_ok']}")
    
    total_404_final = sum(b['404'] for b in stats['worker_stats'])
    total_fail_final = stats['total_fail'] 
    
    report.append(f"‚ùå ID L·ªói (ƒê√£ l∆∞u v√†o {FAIL_ID_FILE}): {total_fail_final}")
    report.append(f"‚ùì ID 404 (Kh√¥ng t√¨m th·∫•y): {total_404_final}")
    report.append("-" * 50)
    report.append("CHI TI·∫æT TH·ªêNG K√ä THEO BATCH (ID OK):")
    
    header = ["Batch", "OK (Total)", "OK (New)", "Time"]
    data_rows = [header]
    for batch in stats['batches']:
        new_ok_count = batch.get('ok_new', batch['ok']) 
        data_rows.append([batch['batch'], batch['ok'], new_ok_count, batch['time']])
        
    for row in data_rows:
        report.append(f"| {str(row[0]).ljust(5)} | {str(row[1]).ljust(10)} | {str(row[2]).ljust(10)} | {str(row[3]).ljust(10)} |")
    
    report.append("="*50)
    report.append("DONE.")
    
    return "\n".join(report)


# --- H√ÄM CH√çNH ---

async def crawl_main():
    start_time_total = time.time()
    
    Path(SAVE_FOLDER).mkdir(exist_ok=True)
    
    # 1. T·∫£i d·ªØ li·ªáu batch cu·ªëi c√πng v√† x√°c ƒë·ªãnh index kh·ªüi ƒë·∫ßu
    next_batch_index, initial_batch_buffer = load_last_partial_batch()
    
    # 2. T·∫£i ID ƒë√£ ho√†n th√†nh v√† l·ªçc ID c·∫ßn ch·∫°y
    completed_ids = load_completed_ids(initial_batch_buffer)
    completed_ids_on_start = len(completed_ids)
    
    # 3. X√°c ƒë·ªãnh ch·∫ø ƒë·ªô ghi file th·ªëng k√™ (W=Ghi ƒë√®, A=Th√™m v√†o)
    file_write_mode = "w"
    run_mode_name = "OVERWRITE"
    if next_batch_index > 1:
        file_write_mode = "a"
        run_mode_name = "APPEND"

    # 4. T√≠nh to√°n offset v√† t·∫£i block ID ti·∫øp theo
    offset = completed_ids_on_start 
    all_ids = load_ids(offset) 
    initial_total_ids = len(all_ids)
    
    if initial_total_ids == 0:
        print("\n==================================")
        print(f"ƒê√£ ƒë·∫°t ƒë·∫øn cu·ªëi file CSV ({INPUT_CSV}). Kh√¥ng c√≤n ID ƒë·ªÉ crawl.")
        return

    ids_to_run = [pid for pid in all_ids if pid not in completed_ids]
    total_ids = len(ids_to_run)
    
    if total_ids == 0:
        print("\n==================================")
        print("T·∫§T C·∫¢ ID TRONG BLOCK HI·ªÜN T·∫†I ƒê√É ƒê∆Ø·ª¢C X·ª¨ L√ù. B·∫ÆT ƒê·∫¶U CHUY·ªÇN BLOCK TI·∫æP THEO.")
        return # N·∫øu t·∫•t c·∫£ ID trong block 2000 ƒë√£ xong, ng∆∞·ªùi d√πng c·∫ßn ch·∫°y l·∫°i ƒë·ªÉ load block ti·∫øp.

    print(f"T·ªïng s·ªë ID ƒë√£ ho√†n th√†nh (Offset): {completed_ids_on_start} ID")
    print(f"ƒê√£ t·∫£i {initial_total_ids} ID ti·∫øp theo t·ª´ file CSV.")
    
    if initial_batch_buffer:
        print(f"S·∫Ω ti·∫øp t·ª•c ƒëi·ªÅn v√†o file {next_batch_index:03d}.json.")
    else:
        print(f"B·∫Øt ƒë·∫ßu ghi file JSON t·ª´ index: {next_batch_index:03d}.")
        
    print(f"Ch·∫ø ƒë·ªô ghi th·ªëng k√™: {run_mode_name}")
    print(f"C·∫ßn x·ª≠ l√Ω l·∫°i {total_ids} ID ‚Äî b·∫Øt ƒë·∫ßu crawl v·ªõi {CONCURRENCY} workers...")

    # Kh·ªüi t·∫°o Queue v√† Stats
    queue = asyncio.Queue()
    for pid in ids_to_run:
        queue.put_nowait(pid)
    
    stats_lock = asyncio.Lock()
    stats = {
        'initial_total_ids': initial_total_ids,
        'completed_ids_on_start': completed_ids_on_start,
        'total_ids': total_ids,
        'total_time': 0,
        'total_ok': 0, 
        'total_404': 0, 
        'total_fail': 0,
        'batches': [],
        'worker_stats': [],
        'lock': stats_lock,
        'workers_done': False 
    }

    # Kh·ªüi ƒë·ªông Workers v√† Saver
    connector = aiohttp.TCPConnector(limit=CONCURRENCY * 2) 
    async with aiohttp.ClientSession(connector=connector, headers=HEADERS) as session:
        
        saver_task = asyncio.create_task(batch_saver(stats, next_batch_index, initial_batch_buffer))
        
        workers = [
            asyncio.create_task(
                worker(
                    f"W{i}", session, queue, stats
                )
            )
            for i in range(CONCURRENCY)
        ]

        await queue.join()

        for _ in workers:
            queue.put_nowait(None)
        
        workers_results = await asyncio.gather(*workers)

    # T·ªïng h·ª£p k·∫øt qu·∫£
    for result in workers_results:
        stats['worker_stats'].append(result)
        stats['total_404'] += result['404']
        await save_fail_ids(result['fail_ids'])
        
    stats['workers_done'] = True
    await saver_task
    stats['total_time'] = time.time() - start_time_total
    
    
    # L∆∞u b√°o c√°o cu·ªëi c√πng
    report_content = format_stats_report(stats, run_mode_name)
    
    try:
        with open(STATS_RESULT_FILE, file_write_mode, encoding="utf-8") as f:
            f.write(report_content)
            
        print("\n" + "="*50)
        print(f"B√ÅO C√ÅO CRAWL ƒê√É L∆ØU V√ÄO: {STATS_RESULT_FILE}")
        print(f"Ch·∫ø ƒë·ªô ghi: {run_mode_name}")
        print(f"T·ªïng th·ªùi gian ch·∫°y: {stats['total_time']:.2f} gi√¢y")
        print("="*50 + "\nDONE.")
    except Exception as e:
        print(f"L·ªñI khi l∆∞u file th·ªëng k√™: {e}")


if __name__ == "__main__":
    asyncio.run(crawl_main())